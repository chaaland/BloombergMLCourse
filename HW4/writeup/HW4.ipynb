{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from load import *\n",
    "from util import * \n",
    "from pegasos import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['figure.figsize'] = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel matrix is given by $K = XX^T \\in \\mathbf{R}^{n\\times n}$. Explicitly in terms of training vectors it's given by \n",
    "\n",
    "$$ K = \n",
    "\\begin{bmatrix}\n",
    "x_1^Tx_1 & x_1^Tx_2 & \\cdots & x_1^Tx_n\\\\\n",
    "x_2^Tx_1 & x_2^Tx_2 & \\cdots & x_2^Tx_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_n^Tx_1 & x_n^Tx_2 & \\cdots & x_n^Tx_n^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The squared Euclidean distance between two vectors is given by $d(x,y) = ||x_i-x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2x_i^Tx_j$. It's clear the Gram matrix contains all the information needed to compute the pairwise distances between training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the regularized least squares objective\n",
    "\n",
    "$$J(w) = ||Xw-y||^2+ \\lambda ||w||^2$$\n",
    "where $\\lambda > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be written as an ordinary least squres problem of the form $\\min ||Aw - b||^2$ where \n",
    "\n",
    "$$ \n",
    "A = \\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix},\n",
    "b=\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is easily solved as \n",
    "$$\n",
    "\\begin{align*}\n",
    "w^\\star &= (A^TA)^{-1}A^Tb\\\\\n",
    "&= \\left(\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}\\right)^{-1}\n",
    "\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "0\n",
    "\\end{bmatrix}\\\\\n",
    "&=(X^TX + \\lambda I)^{-1}X^Ty\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equations for the regularized least squares problem is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X^TX + \\lambda I)w^\\star &= X^Ty\\\\\n",
    "w^\\star &= \\frac{1}{\\lambda} (X^Ty - X^TXw^\\star)\\\\\n",
    "w^\\star &= \\frac{1}{\\lambda} X^T(y - Xw^\\star)\\\\\n",
    "         &= X^T\\alpha\\qquad                //\\ \\alpha := \\frac{1}{\\lambda} (y - Xw)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the optimal weight vector $w^\\star$ is in the span of the data since by the above it is given as a linear combination of the training vectors\n",
    "\n",
    "$$w^\\star = \\sum_{i=1}^n \\alpha_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the weight vector is \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha &= 1/\\lambda(y - Xw^\\star)\\\\\n",
    "\\alpha &= 1/\\lambda(y - XX^T\\alpha)\\\\\n",
    "\\lambda \\alpha &= y - XX^T\\alpha\\\\\n",
    "(XX^T + \\lambda I)\\alpha &= y\\\\\n",
    "\\alpha &= (XX^T + \\lambda I)^{-1}y\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of kernelized ridge regression on the training data is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= Xw^\\star\\\\\n",
    "&= X(X^T\\alpha)\\\\\n",
    "&= XX^T(XX^T + \\lambda I)^{-1}y\\\\\n",
    "&= K(K + \\lambda I)^{-1}y\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction of a new training example we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= x^Tw^\\star\\\\\n",
    "&= \\sum_{i=1}^n \\alpha_i x^Tx_i\\\\\n",
    "&= \\sum_{i=1}^n \\alpha_i k(x, x_i)\\\\\n",
    "&= \\alpha^T k_x\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
