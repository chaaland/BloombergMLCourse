{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from load import *\n",
    "from util import * \n",
    "from pegasos import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['figure.figsize'] = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel matrix is given by $K = XX^T \\in \\mathbf{R}^{n\\times n}$. Explicitly in terms of training vectors it's given by \n",
    "\n",
    "$$ K = \n",
    "\\begin{bmatrix}\n",
    "x_1^Tx_1 & x_1^Tx_2 & \\cdots & x_1^Tx_n\\\\\n",
    "x_2^Tx_1 & x_2^Tx_2 & \\cdots & x_2^Tx_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_n^Tx_1 & x_n^Tx_2 & \\cdots & x_n^Tx_n^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The squared Euclidean distance between two vectors is given by $d(x,y) = ||x_i-x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2x_i^Tx_j$. It's clear the Gram matrix contains all the information needed to compute the pairwise distances between training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the regularized least squares objective\n",
    "\n",
    "$$J(w) = ||Xw-y||^2+ \\lambda ||w||^2$$\n",
    "where $\\lambda > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be written as an ordinary least squres problem of the form $\\min ||Aw - b||^2$ where \n",
    "\n",
    "$$ \n",
    "A = \\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix},\n",
    "b=\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is easily solved as \n",
    "$$\n",
    "\\begin{align*}\n",
    "w^\\star &= (A^TA)^{-1}A^Tb\\\\\n",
    "&= \\left(\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}\\right)^{-1}\n",
    "\\begin{bmatrix}\n",
    "X\\\\\n",
    "\\sqrt{\\lambda} I\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "0\n",
    "\\end{bmatrix}\\\\\n",
    "&=(X^TX + \\lambda I)^{-1}X^Ty\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equations for the regularized least squares problem is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X^TX + \\lambda I)w^\\star &= X^Ty\\\\\n",
    "w^\\star &= \\frac{1}{\\lambda} (X^Ty - X^TXw^\\star)\\\\\n",
    "w^\\star &= \\frac{1}{\\lambda} X^T(y - Xw^\\star)\\\\\n",
    "         &= X^T\\alpha\\qquad                //\\ \\alpha := \\frac{1}{\\lambda} (y - Xw)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the optimal weight vector $w^\\star$ is in the span of the data since by the above it is given as a linear combination of the training vectors\n",
    "\n",
    "$$w^\\star = \\sum_{i=1}^n \\alpha_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the weight vector is \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha &= 1/\\lambda(y - Xw^\\star)\\\\\n",
    "\\alpha &= 1/\\lambda(y - XX^T\\alpha)\\\\\n",
    "\\lambda \\alpha &= y - XX^T\\alpha\\\\\n",
    "(XX^T + \\lambda I)\\alpha &= y\\\\\n",
    "\\alpha &= (XX^T + \\lambda I)^{-1}y\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of kernelized ridge regression on the training data is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= Xw^\\star\\\\\n",
    "&= X(X^T\\alpha)\\\\\n",
    "&= XX^T(XX^T + \\lambda I)^{-1}y\\\\\n",
    "&= K(K + \\lambda I)^{-1}y\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction of a new training example we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= x^Tw^\\star\\\\\n",
    "&= \\sum_{i=1}^n \\alpha_i x^Tx_i\\\\\n",
    "&= \\sum_{i=1}^n \\alpha_i k(x, x_i)\\\\\n",
    "&= \\alpha^T k_x\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function the SVM is given by \n",
    "$$J(w) = \\frac{\\lambda}{2} ||w||^2 + \\frac{1}{n}\\sum_{i=1}^n \\ell_i(w)$$\n",
    "Define $J_i(w) = \\frac{\\lambda}{2} ||w||^2 + \\ell_i(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining $g_i(w) \\in \\partial J_i(w)$ and $v_i(w) \\in \\partial \\ell_i(w)$ by taking a subgradient of $J_i(w)$ we have $\\lambda w + v_i(w) \\in \\partial J_i(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of a subgradient of $J_i(w)$ is\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{E}[g_i(w)] &= \\sum_{i=1}^N p(i)g_i(w)\\\\\n",
    "    &= \\frac{1}{N}\\sum_{i=1}^N g_i(w)\\\\\n",
    "    &\\in \\partial J(w)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Pegasos algorithm with $w^{(1)} = 0$ in the $t$th step we have $w^{(t + 1)} = w^{(t)} - \\eta^{(t)}g_i(w^{(t)})$. We also denote $v^{(t)} = v_i(w^{(t)})$ and want to prove the update rule $w^{(t+1)} = - \\frac{1}{\\lambda t} \\sum_{\\tau=1}^t v^{(\\tau)}$\n",
    "\n",
    "Computing $w^{(2)}$ we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "    w^{(2)} &= w^{(1)} - \\eta^{(t)} g_i(w^{(1)})\\\\\n",
    "    &= -\\frac{1}{\\lambda t} v_i(w^{(1)})\\\\\n",
    "    &= -\\frac{1}{\\lambda t} v^{(1)}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "This shows that the rule holds for $t=2$. Now suppose it holds for all $t$ up to $t+1$ then \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    w^{(t+1)} &= w^{(t)} - \\eta^{(t)} g_i(w^{(t)})\\\\\n",
    "            &= w^{(t)} - \\frac{1}{\\lambda t} g_i(w^{(t)})\\\\\n",
    "            &= w^{(t)} - \\frac{1}{\\lambda t} (\\lambda w^{(t)} + v_i(w^{(t)}))\\\\\n",
    "            &= \\left(\\frac{t-1}{t}\\right) w^{(t)} - \\frac{1}{\\lambda t}v_i(w^{(t)})\\\\\n",
    "            &= \\left(\\frac{t-1}{t}\\right) \\left(- \\frac{1}{\\lambda (t-1)} \\sum_{\\tau=1}^{t-1} v^{(\\tau)}\\right) - \\frac{1}{\\lambda t} v_i(w^{(t)})\\qquad //\\ \\text{induction}\\\\\n",
    "            &= -\\frac{1}{\\lambda t} \\left(\\sum_{\\tau=1}^{t-1} v^{(\\tau)}\\right) - \\frac{1}{\\lambda t} v^{(t)}\\\\\n",
    "            &=-\\frac{1}{\\lambda t} \\sum_{\\tau=1}^{t} v^{(\\tau)}\n",
    "\\end{align*}\n",
    "$$\n",
    "which concludes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining $\\theta^{(t+1)} := \\sum_{\\tau=1}^{t-1} v^{(t)}$ then we can update $w^{(t+1)} := -\\frac{1}{\\lambda t} \\theta^{(t+1)}$. The Pegasos implementation would require that we only update $\\theta$ which requires $\\mathbf{nnz}(x_j)$ operations. To get $w$ we simply multiply by $-\\frac{1}{\\lambda t}$ before returning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Sigma \\succeq 0$ with eigenvector $v$ with eigenvalue $\\sigma$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "v^T\\Sigma v &\\ge 0 \\qquad //\\text{definition of PSD} \\\\\n",
    "v^T (\\sigma v) &\\ge 0 \\\\\n",
    "\\sigma ||v||^2 &\\ge 0\\\\\n",
    "\\sigma &\\ge 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This shows that any eigenvector of a PSD matrix will be nonnegative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Sigma \\succeq 0 \\implies \\Sigma = BB^T$. This follows from the fact that a symmetric matrix has eigenvalue decomposition $\\Sigma = Q\\mathbf{diag}(\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)Q^T$. Since all the eigenvalues are nonnegative we have $\\Sigma = QD^{1/2}D^{T/2}Q^T$ where $D^{1/2} = \\mathbf{diag}\\left(\\sqrt{\\sigma_1}, \\sqrt{\\sigma_2}, \\ldots, \\sqrt{\\sigma_n}\\right)$. We can identify $B= QD^{1/2}$\n",
    "\n",
    "If $\\Sigma = BB^T \\implies \\Sigma\\succeq 0$. This follows immediately from the definition of PSD\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^T\\Sigma x &= x^TBB^Tx\\\\\n",
    "  &= (Bx)^TB^Tx\\\\\n",
    "  &= ||Bx||^2 \\qquad //\\ x^Tx = ||x||^2\\\\\n",
    "  &\\ge 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore $\\Sigma \\succeq 0$ if and only if it has a square root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Sigma \\succ 0$ with eigenvector $v$ and eigenvalue $\\sigma$ the \n",
    "$$\n",
    "\\begin{align*}\n",
    "    v^T\\Sigma v > 0  \\qquad //\\ \\text{def of positive definite}\\\\\n",
    "    v^T(\\sigma v) > 0\\\\\n",
    "    \\sigma||v||^2 > 0\\\\\n",
    "    \\sigma > 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the spectral theorem we can write $\\Sigma = QDQ^T$ where $Q^TQ = I$ and $D = \\mathbf{diag}(\\sigma_1,\\sigma_2,\\ldots,\\sigma_n)$. We can trivially verify that $QD^{-1}Q^T$ is the inverse of $\\Sigma$ since \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    (QD^{-1}Q^T)\\Sigma &= (QD^{-1}Q^T)QDQ^T \\\\\n",
    "    &= QD^{-1}DQ^T\\\\\n",
    "    &=QQ^T\\\\\n",
    "    &= I \\qquad \\blacksquare\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $M \\succeq 0$ and $\\lambda > 0$ then $M+\\lambda I \\succ 0$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  x^T(M+\\lambda I)x &= x^TMx +\\lambda ||x||^2\\\\\n",
    "  &\\ge \\lambda ||x||^2 \\\\\n",
    "  & > 0      \\qquad \\forall x \\ne 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $M \\succeq 0$ and $N\\succ 0$ then $M+N \\succ 0$ which follows immediately form the definition\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    x^T(M+N)x &= x^TMx + x^TNx\\\\\n",
    "    &\\ge x^TNx\\\\\n",
    "    &> 0 \\qquad \\forall x \\ne 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
