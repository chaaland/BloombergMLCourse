{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hinge loss penalty is defined as \n",
    "$$\\ell_1(h, (x_i,y_i)) = \\max_{y\\in \\mathcal{Y}\\backslash \\{y_i\\}}\\left(\\Delta(y_i,y)-m_{i,y}(h)\\right)_+ $$\n",
    "while the generalized hinge loss is defined as \n",
    "$$\\ell_2(h, (x_i,y_i)) = \\max_{y\\in \\mathcal{Y}}\\left[\\Delta(y_i,y)+h(x_i,y)-h(x_i,y_i)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing $\\Delta(y,y) = 0$ then \n",
    "\\begin{align*}\n",
    "\\ell_2(h, (x_i,y_i)) &= \\max_{y\\in \\mathcal{Y}}\\left[\\Delta(y_i,y)+h(x_i,y)-h(x_i,y_i)\\right]\\\\\n",
    "&= \\max \\left(\\Delta(y_i,y_i) + h(x_i,y_i) - h(x_i,y_i), \\max_{y\\in \\mathcal{Y}\\backslash\\{y_i\\}}\\left[\\Delta(y_i,y)+h(x_i,y)-h(x_i,y_i)\\right]\\right)\\\\\n",
    "&= \\max \\left(0, \\max_{y\\in \\mathcal{Y}\\backslash\\{y_i\\}}\\left[\\Delta(y_i,y)+h(x_i,y)-h(x_i,y_i)\\right]\\right)\\\\\n",
    "&= \\max \\left(0, \\max_{y\\in \\mathcal{Y}\\backslash\\{y_i\\}}\\left[\\Delta(y_i,y)-m_{i,y}(h)\\right]\\right)\\\\\n",
    "&= \\max_{y\\in \\mathcal{Y}\\backslash\\{y_i\\}}\\left[\\Delta(y_i,y)-m_{i,y}(h)\\right]_+\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing $m_{i,y}(h) = h(x_i,y_i)-h(x_i,y) \\ge \\Delta(y_i,y)$ then $\\Delta(y_i,y) -m_{i,y}(h) \\le 0$ which implies $\\ell_1(h,(x_i,y_i))=0$. This further implies $\\ell_1(h,(x_i,y_i))=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the above conditions, since $h(x_i,y_i)-h(x_i,y)\\gt 0$ then $h(x_i,y_i)\\gt h(x_i,y)$ we must have that $y_i = \\arg \\max h(x_i,y)$ which means we make a correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\ell_2$ regularized loss function is given by \n",
    "$$J(w) = \\lambda \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\max_{y\\in \\mathcal{Y}}\\left[\\Delta(y_i,y) + \\langle w, \\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $J(w)$ is convex since the first term is the sum of squares and the second terms is the pointwise maximum of an affine function of $w$. Since affine functions are convex, the pointwise maximum of convex functions is convex, and the sum of convex functions is convex, then $J(w)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subgradient of $J(w)$ is given by \n",
    "$$2\\lambda w + 1/n\\sum_{i=1}^n \\Psi(x_i,\\hat{y}_i) - \\Psi(x_i,y_i)$$ where $\\hat{y}_i = \\arg \\max_{y\\in \\mathcal{Y}}\\left[\\Delta(y_i,y) + \\langle w, \\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mathcal{Y} = \\{-1,1\\}$ and $\\Delta(y,\\hat{y})=\\mathbf{1}(y\\ne\\hat{y})$ as well as the compatibility score functions $h(x,1) = g(x)/2$ and $h(x,-1)=-g(x)/2$ the multiclass hinge loss is given by \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(h,(x,y)) &= \\max_{y'\\in\\mathcal{Y}}\\left[\\Delta(y,y') + h(x,y')-h(x,y)\\right]\\\\\n",
    "&= \\max\\left(\\Delta(y,-1) + h(x,-1)-h(x,y), \\Delta(y,1) + h(x,1)-h(x,y)\\right)\\\\\n",
    "&= \\max\\left(\\Delta(y,-1) -g(x)/2-yg(x)/2, \\Delta(y,1) + g(x)/2-yg(x)/2\\right)\\\\\n",
    "&= \\max\\left\\{0,1-yg(x)\\right\\}\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
